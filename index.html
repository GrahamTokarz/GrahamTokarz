<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:site_name" content="Maze">

        <link rel = "stylesheet" type = "text/css" href = "format.css"/>
        <link href="https://fonts.cdnfonts.com/css/seaford" rel="stylesheet">
        <title>ML 4641 Team 18</title>
    </head>
    <body style="background-color: #f2f2f2">
        <div id="Report" style="width: 70%; margin-left: 15%; margin-top: 2%;">
            <h1>Proposal: Machine Learning for Signature Verification</h1>
            <br>
            <h2>Introduction / Background</h2>
            <p>
                Signature forgery is a widespread issue impacting banking, legal, and business sectors.
                Forged signatures can lead to unauthorized access, fraudulent transactions, and significant
                financial losses. Businesses, in particular, can suffer from fraudulent contracts and
                unauthorized transactions, impacting their credibility and operational efficiency. Individuals
                can also experience significant financial loss when forgers steal money from their accounts
                or engage in fraudulent transactions using their forged signatures. Manual detection is
                labor-intensive and prone to errors, necessitating an automated and reliable method for
                verification.
            </p>
            <h3>Literature Review</h3>
            <p>
                Several studies have explored signature verification using machine learning. For example,
                Poddar et al. (2020) proposed a method for signature recognition and forgery detection
                using Convolutional Neural Networks (CNN), the Crest-Trough method, and the SURF
                algorithm. They achieved an accuracy of 85-89% for forgery detection and 90-94% for
                signature recognition. Another study by Justino et al. (2004) evaluated different learning
                strategies and classification methods for offline signature verification. They found that
                support vector machines (SVM) outperformed other methods in scenarios involving
                forgeries. A systematic review by Abdelrahman et al (2021) highlighted the effectiveness of
                deep learning-based models for offline signature verification, consolidating performances
                across various public datasets.
            </p>
            <h3>Dataset Description</h3>
            <p>
                The dataset consists of images of signatures from 30 individuals, each with 5 genuine and 5
                forged signatures. The images are named in a way that differentiates between genuine and
                forged signatures.

                It can be found <a href="https://www.kaggle.com/datasets/divyanshrai/handwritten-signatures/data" target="_blank">here</a>.
            </p>
            <br>
            <h2>Problem Definition</h2>
            <h3>Problem</h3>
            <p>
                The primary problem is to develop a machine learning model that can accurately distinguish
                between genuine and forged signatures.
            </p>
            <h3>Motivation</h3>
            <p>
                Signature verification is important for preventing fraud and ensuring the authenticity of
                documents. An automated system can significantly reduce the time and effort required for
                manual verification while also increasing accuracy and reliability.
            </p>
            <br>
            <h2>Methods (midterm update)</h2>

            <h3>Data preprocessing</h3>
            <ul>
                <li>The image is read by OpenCV.</li>
                <li>The image is converted from BGR to RGB format.</li>
                <li>The image is resized to 224x224 pixels.</li>
                <li>If the image is a genuine signature, it is labeled with a 1.</li>
                <li>If the image is a forgery, it is labeled with a 0.</li>
                <li>The data is then converted into numpy arrays.</li>
                <li>The same function is used to process both the training and test images.</li>
            </ul>


            <h3>Model Description (midtem update)</h3>
            <ol>
                <li>
                    <strong>Base Model:</strong>
                    <ul>
                        <li>Used VGG16 from tensorflow.keras.applications with pre-trained weights from 'imagenet'.</li>
                        <li>Set include_top=False to exclude the top fully connected layers.</li>
                        <li>Defined the input shape as (224, 224, 3).</li>
                    </ul>
                </li>
                <li>
                    <strong>Freeze Base Model:</strong>
                    <ul>
                        <li>We Set base_model.trainable = False to prevent the layers of the VGG16 model from being updated during training.</li>
                    </ul>
                </li>
                <li>
                    <strong>Custom Layers:</strong>
                    <ul>
                        <li><strong>Flatten layer:</strong> Converts the 2D feature maps from the base model to a 1D vector.</li>
                        <li><strong>Dense layer:</strong> Adds a fully connected layer with 128 neurons and ReLU activation.</li>
                        <li><strong>Dropout layer:</strong> Randomly sets 30% of neurons to 0 to prevent overfitting.</li>
                        <li><strong>Output layer:</strong> Adds a fully connected layer with a single neuron and sigmoid activation for binary classification.</li>
                    </ul>
                </li>
                <li>
                    <strong>Model Definition:</strong>
                    <ul>
                        <li>Defined the model with the input from the base model and the output as the custom layers.</li>
                    </ul>
                </li>
                <li>
                    <strong>Model Compilation:</strong>
                    <ul>
                        <li>Compiled the model using RMSprop optimizer.</li>
                        <li>Used binary crossentropy as the loss function.</li>
                        <li>Included accuracy as a metric to evaluate the model performance.</li>
                    </ul>
                </li>
            </ol>
            <br>

            <h2>Discussion of Results (midterm update)</h2>

            <ul>
                <li><strong>Accuracy</strong>: 0.6278</li>
                <li><strong>Precision</strong>: 0.6055</li>
                <li><strong>Recall</strong>: 0.7333</li>
            </ul>

            <h2>Interpretation of Metrics</h2>
            <ul>
                <li>
                    <strong>Accuracy (62.78%)</strong>:
                    <p>Accuracy measures the proportion of correctly predicted instances out of the total instances. An accuracy of 62.78% indicates that the model correctly predicted approximately 63 out of every 100 instances. This metric is helpful as an overall measure but can be misleading in cases of class imbalance.</p>
                </li>
                <li>
                    <strong>Precision (60.55%)</strong>:
                    <p>Precision is the ratio of true positive predictions to the total predicted positives (i.e., how many selected items are relevant). A precision of 60.55% means that out of all instances classified as positive, only about 61% were actually positive. This metric is crucial when the cost of false positives is high.</p>
                </li>
                <li>
                    <strong>Recall (73.33%)</strong>:
                    <p>Recall is the ratio of true positive predictions to the total actual positives (i.e., how many relevant items are selected). A recall of 73.33% indicates that the model identified approximately 73% of the actual positive instances. This metric is essential when the cost of false negatives is high.</p>
                </li>
            </ul>

            <img src="matrix.png" alt="Confusion Matrix" />
            <img src="graphs.png" alt="Confusion Matrix" />
            <br><br>
        </div>
        <div id="References" style="width: 70%; margin-left: 15%; margin-top: 2%;">
            <h2>References</h2>
            <div style="text-indent: -36px; padding-left: 36px;">
                <p>J. Poddar, V. Parikh, and S. K. Bharti, “Offline Signature Recognition and Forgery Detection using Deep Learning,” Procedia Computer Science, vol. 170, pp. 610–617, Jan. 2020, doi: https://doi.org/10.1016/j.procs.2020.03.133.</p>
                <p>S. N. Srihari, Aihua Xu and M. K. Kalera, "Learning strategies and classification methods for off-line signature verification," Ninth International Workshop on Frontiers in Handwriting Recognition, Kokubunji, Japan, 2004, pp. 161-166, doi: 10.1109/IWFHR.2004.61.</p>
                <p>M. M. Hameed, R. Ahmad, M. L. M. Kiah, and G. Murtaza, “Machine learning-based offline signature verification systems: A systematic review,” Signal Processing: Image Communication, vol. 93, p. 116139, Apr. 2021, doi: https://doi.org/10.1016/j.image.2021.116139.</p>
            </div>
            
        </div>
        <div id="Additional">
            <br><br>
            <h2>Gantt Chart</h2>
            <img src="Gantt.png" style="width: 90vw;">
            <br>
            <h2>Contributions Table</h2>
            <table style="width: 50%; margin-left: 25%">
                <tr>
                    <th class="fe">Member</th>
                    <th>Contributions</th>
                </tr>
                <tr>
                    <td class="fe">DB Lee</th>
                    <td>Worked on creating the report and slides.</th>
                </tr>
                <tr>
                    <td class="fe">Ansh Modi</th>
                    <td>Worked on creating the slides, recorded the presentation.</th>
                </tr>
                <tr>
                    <td class="fe">Hassaan Mohammed</th>
                    <td>Worked on creating the report and slides, found literature.</th>
                </tr>
                <tr>
                    <td class="fe">Chaitanya Nifadkar</th>
                    <td>Worked on creating the report and slides.</th>
                </tr>
                <tr>
                    <td class="fel">Joshua Graham Tokarz</th>
                    <td class="le">Worked on the report, creation and formatting of GitHub Pages.</th>
                </tr>
            </table>
        </div>
    </body>
    <footer style="width:50px; height: 10vh;"></footer>
</html>
